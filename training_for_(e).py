
# -*- coding: utf-8 -*-
"""LLM_prompter.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12blFoZDOGTn7x8lvE1lsEqz1lPpz51gD
"""

from peft import PeftModel, PeftConfig
import os
import sys
import argparse
import collections
from colorama import *
from tqdm import tqdm
import numpy as np
import torch
import torch.nn as nn
import collections
from datasets import load_dataset, load_from_disk
from accelerate import Accelerator
from torch.utils.data import DataLoader
import wandb

from transformers import (
    AutoTokenizer,
    AutoConfig,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    GenerationConfig
)

from trl import (
    PPOTrainer,
    PPOConfig,
    AutoModelForCausalLMWithValueHead,
    create_reference_model,
    AutoModelForCausalLMWithValueHead
)
# 生成輸入提示，包含指令、輸入和回覆信息（若有輸入）
def generate_prompt(data_point):
    if data_point["input"]:
        return f"""下方是一個關於任務的指令，以及一個提供與任務相關之資訊的輸入。請撰寫一個能適當地完成該任務指令需求的回覆。
### 指令:
{data_point["instruction"]}
### 輸入:
{data_point["input"]}
### 回覆:
{data_point["output"]}"""
    else:
        return f"""下方是一個關於任務的指令。請撰寫一個能適當地完成該任務指令需求的回覆。
### 輸入:
{data_point["instruction"]}
### 回覆:
{data_point["output"]}"""

# def generate_prompt(data_point):
#     if data_point["input"]:
#         return ("以下是一個描述任務的指令，以及一個與任務資訊相關的輸入。請撰寫一個能適當完成此任務指令的回覆\n\n"
#         f'### 指令：\n{data_point["instruction"]}\n\n### 輸入：\n{data_point["input"]}\n\n'
#         f'### 回覆：\n{data_point["output"]}')
#     else:
#         return ("以下是一個描述任務的指令。請撰寫一個能適當完成此任務指令的回覆\n\n"
#         f'### 指令：\n{data_point["instruction"]}\n\n### 回覆：\n{data_point["output"]}')

# 將提示文本轉換成模型所需的數字表示形式
def tokenize(prompt):
    result = tokenizer(
        prompt,
        truncation=True,
        max_length=CUTOFF_LEN + 1,
        padding="max_length",
    )
    return {
        "input_ids": result["input_ids"][:-1],
        "attention_mask": result["attention_mask"][:-1],
    }

# 生成推理用的提示文本，包含指令和輸入（若有）
def generate_prompt_inference(instruction, input=None):
    if input:
        return f"""下方是一個關於任務的指令，以及一個提供與任務相關之資訊的輸入。請撰寫一個能適當地完成該任務指令需求的回覆。
### 指令:
{instruction}

### 輸入:
{input}

### 回覆:"""
    else:
        return f"""下方是一個關於任務的指令。請撰寫一個能適當地完成該任務指令需求的回覆。
### 輸入:
{instruction}

### 回覆:"""

# def generate_prompt_inference(instruction, input=None):
#     if input:
#         return f"""以下是一個描述任務的指令，以及一個與任務資訊相關的輸入。請撰寫一個能適當完成此任務指令的回覆\n\n
# ### 指令:
# {instruction}

# ### 輸入:
# {input}

# ### 回覆:"""
#     else:
#         return f"""以下是一個描述任務的指令。請撰寫一個能適當完成此任務指令的回覆\n\n
# ### 輸入:
# {instruction}

# ### 回覆:"""

# 進行生成回覆的評估
def evaluate(model,instruction, generation_config, max_len, input=None):
    # 根據指令和輸入生成提示文本
    prompt = generate_prompt_inference(instruction, input)
    # 將提示文本轉換為模型所需的數字表示形式
    inputs = tokenizer(prompt, return_tensors="pt")
    input_ids = inputs["input_ids"].cuda()
    # 使用模型進行生成回覆
    model.eval()
    generation_output = model.generate(
        input_ids=input_ids,
        generation_config=generation_config,
        return_dict_in_generate=True,
        output_scores=True,
        max_new_tokens=max_len,
    )
    # 將生成的回覆解碼並印出
    for s in generation_output.sequences:
        output = tokenizer.decode(s)
        # print(output)
        # print(f"{Fore.GREEN}回覆:{Style.RESET_ALL}")
        # print(output.split("### 回覆:")[1].strip() + '\n')
    return output.split("### 回覆:")[1].strip()


# For reproduction
SEED = 42
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
torch.manual_seed(SEED)
DEVICE = "cpu"
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)
    DEVICE = "cuda:0"

# Reward function

def generate_prompt_inference(instruction, input=None):
    if input:
        return f"""以下是一個描述任務的指令，以及一個與任務資訊相關的輸入。請撰寫一個能適當完成此任務指令的回覆\n\n
### 指令:
{instruction}

### 輸入:
{input}

### 回覆:"""
    else:
        return f"""以下是一個描述任務的指令。請撰寫一個能適當完成此任務指令的回覆\n\n
### 輸入:
{instruction}

### 回覆:"""

def normalize_text(s):
    """Removing articles and punctuation, and standardizing whitespace are all typical text processing steps."""
    import string, re

    def remove_articles(text):
        regex = re.compile(r"\b(a|an|the)\b", re.UNICODE)
        return re.sub(regex, " ", text)

    def white_space_fix(text):
        return " ".join(text.split())

    def remove_punc(text):
        exclude = set(string.punctuation)
        return "".join(ch for ch in text if ch not in exclude)

    def lower(text):
        return text.lower()

    return white_space_fix(remove_articles(remove_punc(lower(s))))

def compute_exact_match(prediction, truth):
    return int(normalize_text(prediction) == normalize_text(truth))

def compute_f1(prediction, truth):
    pred_tokens = normalize_text(prediction).split()
    truth_tokens = normalize_text(truth).split()

    # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise
    if len(pred_tokens) == 0 or len(truth_tokens) == 0:
        return int(pred_tokens == truth_tokens)

    common_tokens = set(pred_tokens) & set(truth_tokens)

    # if there are no common tokens then f1 = 0
    if len(common_tokens) == 0:
        return 0

    prec = len(common_tokens) / len(pred_tokens)
    rec = len(common_tokens) / len(truth_tokens)

    return 2 * (prec * rec) / (prec + rec)

def collator(data):
    return dict((key, [torch.tensor(d[key]) if type(d[key]) is list else d[key] for d in data]) for key in data[0])

def tokenize(sample):
    sample['query'] = '以下是一個描述任務的指令。請撰寫一個能適當完成此任務指令的回覆\n\n input: ' + sample['query'] + ' instruction: Question Answering'
    sample["input_ids"] = prompter_tokenizer(sample["query"], truncation=True, max_length=100)["input_ids"]
    # sample["ref_ids"] = tokenizer(sample["answer"], truncation=True, max_length=100)["input_ids"]

    return sample

def best_of_N_Train(ppo_trainer, fix_LLM, tokenizer, generation_kwargs, N=10):
    padding_side_default = tokenizer.padding_side

    for batch in tqdm(ppo_trainer.dataloader):
        query_tensors = batch["input_ids"]
        #### Get response from SFTModel
        batch['response'] = []
        response_tensors = []
        for n in range(N):
          response_tensor = ppo_trainer.generate(query_tensors, **generation_kwargs)
          response_tensors.append(response_tensor)
          batch["response"].append(tokenizer.batch_decode(response_tensor))
          print('response:', batch['response'][n])

        response_tensors = torch.tensor(response_tensors) # N x BS x max_length

        # Encode prompt, and feed to fix_LLM
        predictions = []
        n_rewards = []
        for n in range(N):
          tokenizer.padding_side = "left"
          response_encoded = tokenizer(batch["response"][n], padding=True, truncation=True, max_length=100, return_tensors='pt') # [tokenizer(x, truncation=True, max_length=100, return_tensors='pt').to(DEVICE) for x in batch['response']]
          tokenizer.padding_side = padding_side_default

          prediction = fix_LLM.generate(**response_encoded, max_new_tokens=20)
          # print(prediction)
          # prediction = [tokenizer.decode(r, skip_special_tokens = True) for r in prediction]
          prediction = tokenizer.batch_decode(prediction, skip_special_tokens=True)

          # print('prediction:', prediction)
          #### Compute reward score
          rewards = [torch.tensor(compute_f1(pred, ref), dtype=float) for (pred, ref) in zip(prediction, batch["answer"])]
          n_rewards.append(rewards)

        n_rewards = torch.tensor(n_rewards) # N x BS
        final_reward = torch.max(n_rewards, dim = 0) # BS
        final_response_idx = torch.argmax(n_rewards, dim = 0) # BS

        final_response_tensors = []
        for i in range(len(final_response_idx)):
          final_response_tensors.append(response_tensors[final_response_idx[i]][i])

        '''
        response_tensors = []
        rewards = []
        for i in range(len(query_tensors)):
          current = -np.inf
          current_response = 0
          for j in range(len(n_rewards)):
            if n_rewards[i][j] > current :
              current = n_rewards[i][j]
              current_response = batch["response"][i][j]
          response_tensors.append(current_response)
          rewards.append(current)
        response_tensors = torch.tensor(response_tensors)
        '''
        #### Run PPO step
        stats = ppo_trainer.step(query_tensors, final_response_tensors, final_reward)
        ppo_trainer.log_stats(stats, batch, rewards)

    #### Save model
    ppo_trainer.save_pretrained("my_ppo_model")

def eval(ppo_trainer, fix_LLM, prompter_tokenizer, fixed_tokenizer, generation_kwargs, dataloader):
    padding_side_default = fixed_tokenizer.padding_side
    tokenizer = AutoTokenizer.from_pretrained(
            "meta-llama/Llama-2-7b-chat-hf",
            cache_dir="./cache"
        )
    score = 0
    for batch in tqdm(dataloader):
        query_tensors = batch["input_ids"]
        query_tensors = [q.to('cuda:0') for q in query_tensors]
        #print(query_tensors)
        #### Get response from SFTModel
        
        responses = []
        for i in range(len(batch["query"])):
            responses.append(evaluate(ppo_trainer.model, batch["query"][i], generation_config, 256))
        # print(responses[i])
    #   response_tensors = ppo_trainer.generate(query_tensors, **generation_kwargs, remove_padding=True)
        
        batch["response"] = responses
        response_tensors = fixed_tokenizer(responses, padding=True, truncation=True, max_length=100, return_tensors='pt')
        response_tensors = {k: v.to('cuda:0') for k, v in response_tensors.items()}
        for i in range(len(batch["response"])):
            batch["response"][i] = batch["response"][i].replace(batch['query'][i], "") + " " +batch['query'][i]
        # print(batch["response"][i])
        
        # Encode prompt, and feed to fix_LLM
        fixed_tokenizer.padding_side = "left"
        response_encoded = fixed_tokenizer(batch["response"], padding=True, truncation=True, max_length=100, return_tensors='pt')  # [tokenizer(x, truncation=True, max_length=100, return_tensors='pt').to(DEVICE) for x in batch['response']]
        fixed_tokenizer.padding_side = padding_side_default
        # response_encoded = response_encoded.to("cpu")
        response_encoded = response_encoded.to("cuda:1")

    #   print(response_encoded)         

        # fix_LLM = fix_LLM.to("cuda:1")
        prediction = fix_LLM.generate(**response_encoded, max_new_tokens=20)
        prediction = fixed_tokenizer.batch_decode(prediction, skip_special_tokens=True)

        # print('prediction:', prediction)
        #### Compute reward score
        rewards = [compute_f1(pred, ref) for (pred, ref) in zip(prediction, batch["answer"])]
        score += sum(rewards)

    return score / len(dataloader)
        



def Train(ppo_trainer, valid_dataloader, fix_LLM, prompter_tokenizer, fixed_tokenizer, generation_kwargs, num_epoch=10):
    padding_side_default = fixed_tokenizer.padding_side
    tokenizer = AutoTokenizer.from_pretrained(
            "meta-llama/Llama-2-7b-chat-hf",
            cache_dir="./cache"
        )
    for epoch in range(num_epoch):
      for batch in tqdm(ppo_trainer.dataloader):
          query_tensors = batch["input_ids"]
          query_tensors = [q.to('cuda:0') for q in query_tensors]
          #print(query_tensors)
          #### Get response from SFTModel
          
          responses = []
          for i in range(len(batch["query"])):
            responses.append(evaluate(ppo_trainer.model, batch["query"][i], generation_config, 256))
            # print(responses[i])
        #   response_tensors = ppo_trainer.generate(query_tensors, **generation_kwargs, remove_padding=True)
          
          batch["response"] = responses
          response_tensors = fixed_tokenizer(responses, padding=True, truncation=True, max_length=100, return_tensors='pt')
          response_tensors = {k: v.to('cuda:0') for k, v in response_tensors.items()}
          for i in range(len(batch["response"])):
            batch["response"][i] = batch["response"][i].replace(batch['query'][i], "") + " " +batch['query'][i]
            # print(batch["response"][i])
          
          # Encode prompt, and feed to fix_LLM
          fixed_tokenizer.padding_side = "left"
          response_encoded = fixed_tokenizer(batch["response"], padding=True, truncation=True, max_length=100, return_tensors='pt')  # [tokenizer(x, truncation=True, max_length=100, return_tensors='pt').to(DEVICE) for x in batch['response']]
          fixed_tokenizer.padding_side = padding_side_default
          # response_encoded = response_encoded.to("cpu")
          response_encoded = response_encoded.to("cuda:1")

        #   print(response_encoded)         
          fix_LLM.eval()
          # fix_LLM = fix_LLM.to("cuda:1")
          prediction = fix_LLM.generate(**response_encoded, max_new_tokens=20)
          prediction = fixed_tokenizer.batch_decode(prediction, skip_special_tokens=True)

          # print('prediction:', prediction)
          #### Compute reward score
          rewards = [torch.tensor(compute_f1(pred, ref), dtype=float).to('cuda:0') for (pred, ref) in zip(prediction, batch["answer"])]

          #### Run PPO step
          response_tensor_list = []
          for i in range(len(batch["response"])):
            response_tensor_list.append(response_tensors['input_ids'][i][response_tensors['attention_mask'][i] == 1])
          stats = ppo_trainer.step(query_tensors, response_tensor_list, rewards)
          ppo_trainer.log_stats(stats, batch, rewards)

      #### Save model
      eval_score = eval(ppo_trainer, fix_LLM, prompter_tokenizer, fixed_tokenizer, generation_kwargs, valid_dataloader)
      wandb.log({"eval_score": eval_score})
      ppo_trainer.save_pretrained(f"ppo_add_eval_model_epoch_{epoch}")

if __name__ == "__main__":
    # Training configuration setup
    wandb.login()

    config = PPOConfig(
        model_name="meta-llama/Llama-2-7b-chat-hf",
        learning_rate=1.41e-5,
        batch_size=100,
        remove_unused_columns = False,
        ratio_threshold = 10,
    )

    run = wandb.init(
        # Set the project where this run will be logged
        project="RL-project",
        name = "ppo_add_eval",
        # Track hyperparameters and run metadata
        config={
            "learning_rate": config.learning_rate,
            "epochs": 50,
        },
    )
    
    #tokenizer = AutoTokenizer.from_pretrained(config.model_name, device_map={"": Accelerator().local_process_index})
    prompter_tokenizer = AutoTokenizer.from_pretrained(config.model_name, device_map="cuda:0")
    fixed_tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-chat-hf", device_map="cuda:1")   # why put tokenizer on gpu (tokenizer alway works on cpu)
    prompter_tokenizer.pad_token_id = prompter_tokenizer.eos_token_id
    fixed_tokenizer.pad_token = fixed_tokenizer.bos_token

    # Generation_kwargs
    generation_kwargs = {
        "max_new_tokens": 256,
        "top_p": 0.3,
        "do_sample": True,
        "pad_token_id": prompter_tokenizer.eos_token_id,
        "temperature": 0.3, # Need to modify
        "num_beams":1,
        "no_repeat_ngram_size": 3,
    }

    nf4_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True,
        bnb_4bit_compute_dtype=torch.float16
    )

    sft_model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-chat-hf", cache_dir="./cache", load_in_4bit=True, quantization_config=nf4_config,device_map='cuda:0') # peft_model_id is the path to sft adapters
    # Build model
    sft_model = PeftModel.from_pretrained(sft_model, "./lora_RL")
    model = AutoModelForCausalLMWithValueHead.from_pretrained(sft_model, cache_dir="./cache", device_map='cuda:0').to("cuda:0")
    # perf_config = PeftConfig.from_pretrained("./lora_RL")    #model = AutoModelForCausalLMWithValueHead.from_pretrained("./merge_model").to("cuda:0")
    # model = PeftModel.from_pretrained(model, "./lora_RL")
    model.bfloat16()
    fix_LLM = AutoModelForCausalLMWithValueHead.from_pretrained("meta-llama/Llama-2-7b-chat-hf", cache_dir="./cache", load_in_4bit=True, quantization_config=nf4_config, device_map='cuda:1').to("cuda:1")
    fix_LLM.bfloat16()
    # load dataset
    train_data = load_dataset("xiyuez/im-feeling-curious", split="train[:90%]").rename_column("question","query")
    valid_data = load_dataset("xiyuez/im-feeling-curious", split="train[90%:95%]").rename_column("question","query")
    test_data = load_dataset("xiyuez/im-feeling-curious", split="train[95%:]").rename_column("question","query")
    train_dataset = train_data.map(tokenize)

    valid_dataset = valid_data.map(tokenize)
    test_dataset = test_data.map(tokenize)

    valid_dataloader = DataLoader(valid_dataset)
    test_dataloader = DataLoader(test_dataset)

    max_len = 256
    temperature = 0.1  # 設定生成回覆的隨機度，值越小生成的回覆越穩定
    top_p = 0.3  # Top-p (nucleus) 抽樣的機率閾值，用於控制生成回覆的多樣性
    num_beams = 1  # 設定束搜索 (beam search) 的束寬
    no_repeat_ngram_size = 3 
    generation_config = GenerationConfig(
        do_sample=True,
        temperature=temperature,
        num_beams=num_beams,
        top_p=top_p,
        no_repeat_ngram_size=no_repeat_ngram_size,
    )
    tokenizer = AutoTokenizer.from_pretrained(
        "meta-llama/Llama-2-7b-chat-hf",
        cache_dir="./cache"
    )
    tokenizer.pad_token_id = tokenizer.eos_token_id
    # evaluate(model, input(f"\n{'-'*10}\n{Fore.BLUE}指令: {Style.RESET_ALL}"), generation_config, max_len)
    
    # Trainer setup
    ppo_trainer = PPOTrainer(
        model=model,
        config=config,
        dataset=train_dataset,
        tokenizer=prompter_tokenizer,
        data_collator=collator
    )
    print(f"-" * 50)
    print("PPO device:", ppo_trainer.current_device)

    ppo_trainer.model = ppo_trainer.model.to(ppo_trainer.current_device)

    # eval_score = eval(ppo_trainer, fix_LLM, prompter_tokenizer, fixed_tokenizer, generation_kwargs, valid_dataloader)
    # print(f"eval_score: {eval_score}")
    # ppo_trainer.ref_model = ppo_trainer.ref_model.to(ppo_trainer.current_device)
    Train(ppo_trainer, valid_dataloader, fix_LLM, prompter_tokenizer, fixed_tokenizer, generation_kwargs)

    # Testing
    testing_score = eval(ppo_trainer, fix_LLM, prompter_tokenizer, fixed_tokenizer, generation_kwargs, test_dataloader)
    print(f"testing_score: {testing_score}")